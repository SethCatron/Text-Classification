{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3c677a",
   "metadata": {},
   "source": [
    "# Imports and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f68c5a",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency - Inverse Document Frequency) (frequency of a word wirthin the document X frequency of a word across the documents) is an information retrieval and information extraction subtask which aims to express the importance of a word to a document which is part of a colection of documents which we usually name a corpus. It is usually used by some search engines to help them obtain better results which are more relevant to a specific query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ca25fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "\n",
    "%pip install pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pd.set_option(\"max_rows\", 600)\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a83a6b9",
   "metadata": {},
   "source": [
    "# Create a TfidfVectorizer using the spam files in GP and ham files for kitchen-l and lokay-m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3993b0a0",
   "metadata": {},
   "source": [
    "### Set Directory Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1613ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38a42",
   "metadata": {},
   "source": [
    "### Make list of .tar file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0e2804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/kitchen-l.tar', '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/lokay-m.tar', '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/GP.tar']\n",
      "['kitchen-l', 'lokay-m', 'GP']\n",
      "('kitchen-l.tar', 'lokay-m.tar', 'GP.tar')\n",
      "['kitchen-l']\n",
      "['lokay-m']\n",
      "['GP']\n",
      "['/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/kitchen-l.tar', '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/lokay-m.tar']\n",
      "['/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/GP.tar']\n"
     ]
    }
   ],
   "source": [
    "# Then we’re going to use glob and Path to make a list of all the filepaths in that directory and a \n",
    "# list of all the short story titles.\n",
    "tar_files = glob.glob(f\"{directory_path}/*.tar\")\n",
    "print(tar_files)\n",
    "tar_titles = [Path(tar).stem for tar in tar_files]\n",
    "print(tar_titles)\n",
    "tar_list = tuple(f\"{i}.tar\" for i in tar_titles)\n",
    "print(tar_list)\n",
    "\n",
    "# Create empty tar lists so that they can be appended to and be used in the find_tfidf function below\n",
    "tar_list0 = []\n",
    "tar_list1 = []\n",
    "tar_list2 = []\n",
    "\n",
    "# Append the tar_list values to the individual lists for each ham/spam file representation\n",
    "tar_list0.append(tar_titles[0])\n",
    "tar_list1.append(tar_titles[1])\n",
    "tar_list2.append(tar_titles[2])\n",
    "print(tar_list0) # kitchen-l (ham)\n",
    "print(tar_list1) # lokay-m (ham)\n",
    "print(tar_list2) # GP (spam)\n",
    "tar_list3 = []\n",
    "tar_list4 = []\n",
    "tar_list3.append(tar_files[0])\n",
    "tar_list3.append(tar_files[1])\n",
    "tar_list4.append(tar_files[2])\n",
    "print(tar_list3)\n",
    "print(tar_list4)\n",
    "# unpacks the tar files so that I can grab a specific items from each ham or spam file\n",
    "!tar -xf '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/kitchen-l.tar'\n",
    "!tar -xf '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/lokay-m.tar'\n",
    "!tar -xf '/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/GP.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f1413",
   "metadata": {},
   "source": [
    "### os.walk to grab all of the files names, and iterate through the list to get which ones I need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5e323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kitchen-l/_americas_gas\n",
      "./kitchen-l/_americas_portland\n",
      "./kitchen-l/_americas_netco_canada\n",
      "./kitchen-l/_americas_netco_eol\n",
      "./kitchen-l/_americas_it\n",
      "./kitchen-l/_americas_netco_it\n",
      "./kitchen-l/_americas_bridgeline\n",
      "./kitchen-l/_americas_business_plans\n",
      "./kitchen-l/_americas_press1\n",
      "./kitchen-l/_americas_netco_reg\n",
      "./kitchen-l/_americas_sec\n",
      "./kitchen-l/_americas_legal\n",
      "./kitchen-l/_americas_ooc\n",
      "./kitchen-l/_americas_netco_restart\n",
      "./kitchen-l/_americas_hpl\n",
      "./kitchen-l/_americas_eol\n",
      "./kitchen-l/_americas_tax\n",
      "./kitchen-l/_americas_turbines_trading\n",
      "./kitchen-l/_americas_netco_legal\n",
      "./kitchen-l/_americas_sec_media\n",
      "./kitchen-l/_americas_restructuring\n",
      "./kitchen-l/_americas_mrha_ooc\n",
      "./kitchen-l/_americas_south_america\n",
      "./kitchen-l/_americas_tech_services\n",
      "./kitchen-l/_americas_east_power\n",
      "./kitchen-l/_americas_asset_marketing\n",
      "./kitchen-l/_americas_turbines\n",
      "./kitchen-l/_americas_mrha\n",
      "./kitchen-l/_americas_vp_prc\n",
      "./kitchen-l/_americas_board\n",
      "./kitchen-l/_americas_canada\n",
      "./kitchen-l/_americas_esvl\n",
      "./kitchen-l/_americas_netco_portland\n",
      "./kitchen-l/_americas_hr\n",
      "./kitchen-l/_americas_julie\n",
      "./kitchen-l/_americas_netco_hr\n",
      "./kitchen-l/_americas_regulatory\n",
      "./kitchen-l/_americas_finance\n",
      "./kitchen-l/_americas_press\n",
      "./kitchen-l/_americas_europe\n",
      "./kitchen-l/_americas_culture\n",
      "./kitchen-l/_americas_rac\n",
      "./kitchen-l/_americas_tammie\n",
      "./kitchen-l/_americas_netco_credit\n",
      "./kitchen-l/_americas_earnings\n",
      "./kitchen-l/_americas_mexico\n",
      "./kitchen-l/_americas_ees\n",
      "./kitchen-l\n",
      "./GP/part12\n",
      "./GP/part3\n",
      "./GP/part4\n",
      "./GP/part5\n",
      "./GP/part2\n",
      "./GP/part10\n",
      "./GP/part11\n",
      "./GP/part9\n",
      "./GP/part7\n",
      "./GP/part1\n",
      "./GP/part6\n",
      "./GP/part8\n",
      "./GP\n",
      "./.ipynb_checkpoints\n",
      "./lokay-m/corporate\n",
      "./lokay-m/personal\n",
      "./lokay-m/tw_commercial_group\n",
      "./lokay-m/publications\n",
      "./lokay-m/training\n",
      "./lokay-m/articles\n",
      "./lokay-m/hea_nesa\n",
      "./lokay-m/kim_s_files\n",
      "./lokay-m/enron_travel_club\n",
      "./lokay-m/systems\n",
      "./lokay-m/enron_t_s\n",
      "./lokay-m\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pathstr = \"/Users/sethjohnston/Desktop/TEXAS A&M CLASSES/Final Semester/CSCE 489/HW4/\"\n",
    "strg1 = \"./GP/part1\"\n",
    "strg2 = \"./GP/part2\"\n",
    "strg3 = \"./GP/part3\"\n",
    "strg4 = \"./GP/part4\"\n",
    "strg5 = \"./GP/part5\"\n",
    "strg6 = \"./GP/part6\"\n",
    "strg7 = \"./GP/part7\"\n",
    "strg8 = \"./GP/part8\"\n",
    "strg9 = \"./GP/part9\"\n",
    "strg10 = \"./GP/part10\"\n",
    "strg11 = \"./GP/part11\"\n",
    "strg12 = \"./GP/part12\"\n",
    "strg13 = \"./kitchen-l\"\n",
    "strg14 = \"./lokay-m\"\n",
    "\n",
    "xtrain = [] # GP spam 1-9 & kitchen ham\n",
    "ytrain = [] # append 'spam' or 'ham'\n",
    "xvalid = [] # GP spam 10-12 & lokay ham\n",
    "yvalid = [] # append 'spam' or 'ham'\n",
    "\n",
    "for root, dirs, files in os.walk(\".\", topdown=False):\n",
    "    print(root)\n",
    "    for name in files:\n",
    "        if root.endswith('part1'):\n",
    "            xtrain.append(pathstr + \"GP/part1/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg2 in root:\n",
    "            xtrain.append(pathstr + \"GP/part2/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg3 in root:\n",
    "            xtrain.append(pathstr + \"GP/part3/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg4 in root:\n",
    "            xtrain.append(pathstr + \"GP/part4/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg5 in root:\n",
    "            xtrain.append(pathstr + \"GP/part5/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg6 in root:\n",
    "            xtrain.append(pathstr + \"GP/part6/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg7 in root:\n",
    "            xtrain.append(pathstr + \"GP/part7/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg8 in root:\n",
    "            xtrain.append(pathstr + \"GP/part8/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg9 in root:\n",
    "            xtrain.append(pathstr + \"GP/part9/\" + name)\n",
    "            ytrain.append('spam')\n",
    "        elif strg13 in root:\n",
    "            xtrain.append(pathstr[0:-1] + root[1:] + \"/\" + name)\n",
    "            ytrain.append('ham')\n",
    "        elif strg10 in root:\n",
    "            xvalid.append(pathstr + \"GP/part10/\" + name)\n",
    "            yvalid.append('spam')\n",
    "        elif strg11 in root:\n",
    "            xvalid.append(pathstr + \"GP/part11/\" + name)\n",
    "            yvalid.append('spam')\n",
    "        elif strg12 in root:\n",
    "            xvalid.append(pathstr + \"GP/part12/\" + name)\n",
    "            yvalid.append('spam')\n",
    "        elif strg14 in root:\n",
    "            xvalid.append(pathstr[0:-1] + root[1:] + \"/\" + name)\n",
    "            yvalid.append('ham')\n",
    "         \n",
    "#     print(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb24ea",
   "metadata": {},
   "source": [
    "### TF-IDF function which can be used for the spam and ham files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1597ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf(dataset):\n",
    "    tfIdfVectorizer=TfidfVectorizer(input='filename', decode_error='ignore', max_features=25)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(dataset) # train to recognize vocabulary\n",
    "#     using pandas dataframe to look at the top 25 values\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    print (df.head(25)) \n",
    "    return tfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893852f5",
   "metadata": {},
   "source": [
    "### Run TF-IDF on the ham and spam files (Kitchen-l, lokay-m, GP), basically training the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80799ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           TF-IDF\n",
      "enron    0.678181\n",
      "the      0.444605\n",
      "to       0.311724\n",
      "cn       0.264566\n",
      "com      0.233956\n",
      "of       0.200689\n",
      "and      0.185681\n",
      "in       0.158634\n",
      "from     0.087002\n",
      "for      0.086544\n",
      "20       0.064327\n",
      "content  0.036844\n",
      "http     0.004304\n",
      "face     0.001126\n",
      "size     0.000976\n",
      "3d       0.000794\n",
      "html     0.000518\n",
      "border   0.000327\n",
      "tr       0.000313\n",
      "font     0.000145\n",
      "br       0.000136\n",
      "style    0.000100\n",
      "td       0.000027\n",
      "width    0.000005\n",
      "nbsp     0.000000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = find_tfidf(tar_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3173a78",
   "metadata": {},
   "source": [
    "### Run TF-IDF On 1 ham file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa88364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "to          0.492068\n",
      "in          0.295241\n",
      "the         0.295241\n",
      "we          0.246034\n",
      "louise      0.246034\n",
      "kitchen     0.246034\n",
      "laura       0.196827\n",
      "of          0.196827\n",
      "cn          0.196827\n",
      "your        0.196827\n",
      "enron       0.196827\n",
      "have        0.147620\n",
      "give        0.147620\n",
      "from        0.147620\n",
      "luce        0.147620\n",
      "me          0.147620\n",
      "and         0.147620\n",
      "if          0.098414\n",
      "message     0.098414\n",
      "na          0.098414\n",
      "ou          0.098414\n",
      "recipients  0.098414\n",
      "email       0.098414\n",
      "do          0.098414\n",
      "discuss     0.098414\n"
     ]
    }
   ],
   "source": [
    "hamExample = find_tfidf(xtrain[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e57343",
   "metadata": {},
   "source": [
    "### Run TF-IDF On 1 spam file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ca2e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "br          0.901498\n",
      "html        0.169031\n",
      "to          0.112687\n",
      "http        0.112687\n",
      "mimeole     0.112687\n",
      "104         0.112687\n",
      "priority    0.112687\n",
      "info        0.112687\n",
      "href        0.112687\n",
      "content     0.112687\n",
      "ceskabar    0.112687\n",
      "on          0.056344\n",
      "ph          0.056344\n",
      "paliourg    0.056344\n",
      "order       0.056344\n",
      "opixmgbgfe  0.056344\n",
      "only        0.056344\n",
      "00          0.056344\n",
      "oct         0.056344\n",
      "normal      0.056344\n",
      "msmail      0.056344\n",
      "microsoft   0.056344\n",
      "med1cat     0.056344\n",
      "me          0.056344\n",
      "mime        0.056344\n"
     ]
    }
   ],
   "source": [
    "spamExample = find_tfidf(xtrain[8000:8001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed120e7c",
   "metadata": {},
   "source": [
    "### Transform the TF-IDF vectorizer with the xtrain and xvalid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345a10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedXTrain =  vectorizer.transform(xtrain) # ytrain set\n",
    "transformedXValidation = vectorizer.transform(xvalid) # xvalid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb22491",
   "metadata": {},
   "source": [
    "#  Train a RandomForestClassifier using the parts 1-9 spam emails and kitchen-l ham emails. Use 100 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77173fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(transformedXTrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c468c30",
   "metadata": {},
   "source": [
    "# Using your RandomForestClassifier, predict the emails in GP parts 10-12 and lokay-m. Display the number of true positive (spam), false positive, true negative (ham) and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babb77ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of True Negatives:  2316\n",
      "# of False Positives:  48\n",
      "# of False Negatives:  0\n",
      "# of True Positives:  3423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.98      0.99      2364\n",
      "        spam       0.99      1.00      0.99      3423\n",
      "\n",
      "    accuracy                           0.99      5787\n",
      "   macro avg       0.99      0.99      0.99      5787\n",
      "weighted avg       0.99      0.99      0.99      5787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(transformedXValidation)\n",
    "# Confusion Matrix and Classification Report\n",
    "tn, fp, fn, tp = confusion_matrix(yvalid, ypred).ravel()  # flattening the array and list expansion\n",
    "print(\"# of True Negatives: \", tn)\n",
    "print(\"# of False Positives: \", fp)\n",
    "print(\"# of False Negatives: \", fn)\n",
    "print(\"# of True Positives: \", tp)\n",
    "\n",
    "print(classification_report(yvalid, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff43390",
   "metadata": {},
   "source": [
    "# Again, predict the emails in GP parts 10-12 and lokay-m. Display the number of true positive (spam), false positive, true negative (ham) and false negatives, this time using stopword list attribute in the TF-IDF vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607248a4",
   "metadata": {},
   "source": [
    "### Create a Stopword list by importing relevant text files and appending to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8248bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424\n",
      "['a', 'abbr', 'acronym', 'address', 'applet', 'area', 'article', 'aside', 'audio', 'b', 'base', 'basefont', 'bdi', 'bdo', 'big', 'blockquote', 'body', 'br', 'button', 'canvas', 'caption', 'center', 'cite', 'code', 'col', 'colgroup', 'data', 'datalist', 'dd', 'del', 'details', 'dfn', 'dialog', 'dir', 'div', 'dl', 'dt', 'em', 'embed', 'fieldset', 'figcaption', 'figure', 'font', 'footer', 'form', 'frame', 'frameset', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'head', 'header', 'hr', 'href', 'html', 'i', 'iframe', 'img', 'input', 'ins', 'kbd', 'label', 'legend', 'li', 'link', 'main', 'map', 'mark', 'menu', 'menuitem', 'meta', 'meter', 'nav', 'noframes', 'noscript', 'object', 'ol', 'optgroup', 'option', 'output', 'p', 'param', 'picture', 'pre', 'progress', 'q', 'rp', 'rt', 'ruby', 's', 'samp', 'script', 'section', 'select', 'small', 'source', 'span', 'strike', 'strong', 'style', 'sub', 'summary', 'sup', 'svg', 'table', 'tbody', 'td', 'template', 'textarea', 'tfoot', 'th', 'thead', 'time', 'title', 'tr', 'track', 'tt', 'u', 'ul', 'var', 'video', 'wbr', \"'ll\", \"'tis\", \"'twas\", \"'ve\", '10', '39', 'a', \"a's\", 'able', 'ableabout', 'about', 'above', 'abroad', 'abst', 'accordance', 'according', 'accordingly', 'across', 'act', 'actually', 'ad', 'added', 'adj', 'adopted', 'ae', 'af', 'affected', 'affecting', 'affects', 'after', 'afterwards', 'ag', 'again', 'against', 'ago', 'ah', 'ahead', 'ai', \"ain't\", 'aint', 'al', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'announce', 'another', 'any', 'anybody', 'anyhow', 'anymore', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'ao', 'apart', 'apparently', 'appear', 'appreciate', 'appropriate', 'approximately', 'aq', 'ar', 'are', 'area', 'areas', 'aren', \"aren't\", 'arent', 'arise', 'around', 'arpa', 'as', 'aside', 'ask', 'asked', 'asking', 'asks', 'associated', 'at', 'au', 'auth', 'available', 'aw', 'away', 'awfully', 'az', 'b', 'ba', 'back', 'backed', 'backing', 'backs', 'backward', 'backwards', 'bb', 'bd', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'began', 'begin', 'beginning', 'beginnings', 'begins', 'behind', 'being', 'beings', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'bf', 'bg', 'bh', 'bi', 'big', 'bill', 'billion', 'biol', 'bj', 'bm', 'bn', 'bo', 'both', 'bottom', 'br', 'brief', 'briefly', 'bs', 'bt', 'but', 'buy', 'bv', 'bw', 'by', 'bz', 'c', \"c'mon\", \"c's\", 'ca', 'call', 'came', 'can', \"can't\", 'cannot', 'cant', 'caption', 'case', 'cases', 'cause', 'causes', 'cc', 'cd', 'certain', 'certainly', 'cf', 'cg', 'ch', 'changes', 'ci', 'ck', 'cl', 'clear', 'clearly', 'click', 'cm', 'cmon', 'cn', 'co', 'co.', 'com', 'come', 'comes', 'computer', 'con', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'copy', 'corresponding', 'could', \"could've\", 'couldn', \"couldn't\", 'couldnt', 'course', 'cr', 'cry', 'cs', 'cu', 'currently', 'cv', 'cx', 'cy', 'cz', 'd', 'dare', \"daren't\", 'darent', 'date', 'de', 'dear', 'definitely', 'describe', 'described', 'despite', 'detail', 'did', 'didn', \"didn't\", 'didnt', 'differ', 'different', 'differently', 'directly', 'dj', 'dk', 'dm', 'do', 'does', 'doesn', \"doesn't\", 'doesnt', 'doing', 'don', \"don't\", 'done', 'dont', 'doubtful', 'down', 'downed', 'downing', 'downs', 'downwards', 'due', 'during', 'dz', 'e', 'each', 'early', 'ec', 'ed', 'edu', 'ee', 'effect', 'eg', 'eh', 'eight', 'eighty', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'end', 'ended', 'ending', 'ends', 'enough', 'entirely', 'er', 'es', 'especially', 'et', 'et-al', 'etc', 'even', 'evenly', 'ever', 'evermore', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'face', 'faces', 'fact', 'facts', 'fairly', 'far', 'farther', 'felt', 'few', 'fewer', 'ff', 'fi', 'fifteen', 'fifth', 'fifty', 'fify', 'fill', 'find', 'finds', 'fire', 'first', 'five', 'fix', 'fj', 'fk', 'fm', 'fo', 'followed', 'following', 'follows', 'for', 'forever', 'former', 'formerly', 'forth', 'forty', 'forward', 'found', 'four', 'fr', 'free', 'from', 'front', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthermore', 'furthers', 'fx', 'g', 'ga', 'gave', 'gb', 'gd', 'ge', 'general', 'generally', 'get', 'gets', 'getting', 'gf', 'gg', 'gh', 'gi', 'give', 'given', 'gives', 'giving', 'gl', 'gm', 'gmt', 'gn', 'go', 'goes', 'going', 'gone', 'good', 'goods', 'got', 'gotten', 'gov', 'gp', 'gq', 'gr', 'great', 'greater', 'greatest', 'greetings', 'group', 'grouped', 'grouping', 'groups', 'gs', 'gt', 'gu', 'gw', 'gy', 'h', 'had', \"hadn't\", 'hadnt', 'half', 'happens', 'hardly', 'has', 'hasn', \"hasn't\", 'hasnt', 'have', 'haven', \"haven't\", 'havent', 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'hed', 'hell', 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'heres', 'hereupon', 'hers', 'herself', 'herse”', 'hes', 'hi', 'hid', 'high', 'higher', 'highest', 'him', 'himself', 'himse”', 'his', 'hither', 'hk', 'hm', 'hn', 'home', 'homepage', 'hopefully', 'how', \"how'd\", \"how'll\", \"how's\", 'howbeit', 'however', 'hr', 'ht', 'htm', 'html', 'http', 'hu', 'hundred', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'i.e.', 'id', 'ie', 'if', 'ignored', 'ii', 'il', 'ill', 'im', 'immediate', 'immediately', 'importance', 'important', 'in', 'inasmuch', 'inc', 'inc.', 'indeed', 'index', 'indicate', 'indicated', 'indicates', 'information', 'inner', 'inside', 'insofar', 'instead', 'int', 'interest', 'interested', 'interesting', 'interests', 'into', 'invention', 'inward', 'io', 'iq', 'ir', 'is', 'isn', \"isn't\", 'isnt', 'it', \"it'd\", \"it'll\", \"it's\", 'itd', 'itll', 'its', 'itself', 'itse”', 'ive', 'j', 'je', 'jm', 'jo', 'join', 'jp', 'just', 'k', 'ke', 'keep', 'keeps', 'kept', 'keys', 'kg', 'kh', 'ki', 'kind', 'km', 'kn', 'knew', 'know', 'known', 'knows', 'kp', 'kr', 'kw', 'ky', 'kz', 'l', 'la', 'large', 'largely', 'last', 'lately', 'later', 'latest', 'latter', 'latterly', 'lb', 'lc', 'least', 'length', 'less', 'lest', 'let', \"let's\", 'lets', 'li', 'like', 'liked', 'likely', 'likewise', 'line', 'little', 'lk', 'll', 'long', 'longer', 'longest', 'look', 'looking', 'looks', 'low', 'lower', 'lr', 'ls', 'lt', 'ltd', 'lu', 'lv', 'ly', 'm', 'ma', 'made', 'mainly', 'make', 'makes', 'making', 'man', 'many', 'may', 'maybe', \"mayn't\", 'maynt', 'mc', 'md', 'me', 'mean', 'means', 'meantime', 'meanwhile', 'member', 'members', 'men', 'merely', 'mg', 'mh', 'microsoft', 'might', \"might've\", \"mightn't\", 'mightnt', 'mil', 'mill', 'million', 'mine', 'minus', 'miss', 'mk', 'ml', 'mm', 'mn', 'mo', 'more', 'moreover', 'most', 'mostly', 'move', 'mp', 'mq', 'mr', 'mrs', 'ms', 'msie', 'mt', 'mu', 'much', 'mug', 'must', \"must've\", \"mustn't\", 'mustnt', 'mv', 'mw', 'mx', 'my', 'myself', 'myse”', 'mz', 'n', 'na', 'name', 'namely', 'nay', 'nc', 'nd', 'ne', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needed', 'needing', \"needn't\", 'neednt', 'needs', 'neither', 'net', 'netscape', 'never', 'neverf', 'neverless', 'nevertheless', 'new', 'newer', 'newest', 'next', 'nf', 'ng', 'ni', 'nine', 'ninety', 'nl', 'no', 'no-one', 'nobody', 'non', 'none', 'nonetheless', 'noone', 'nor', 'normally', 'nos', 'not', 'noted', 'nothing', 'notwithstanding', 'novel', 'now', 'nowhere', 'np', 'nr', 'nu', 'null', 'number', 'numbers', 'nz', 'o', 'obtain', 'obtained', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'older', 'oldest', 'om', 'omitted', 'on', 'once', 'one', \"one's\", 'ones', 'only', 'onto', 'open', 'opened', 'opening', 'opens', 'opposite', 'or', 'ord', 'order', 'ordered', 'ordering', 'orders', 'org', 'other', 'others', 'otherwise', 'ought', \"oughtn't\", 'oughtnt', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'owing', 'own', 'p', 'pa', 'page', 'pages', 'part', 'parted', 'particular', 'particularly', 'parting', 'parts', 'past', 'pe', 'per', 'perhaps', 'pf', 'pg', 'ph', 'pk', 'pl', 'place', 'placed', 'places', 'please', 'plus', 'pm', 'pmid', 'pn', 'point', 'pointed', 'pointing', 'points', 'poorly', 'possible', 'possibly', 'potentially', 'pp', 'pr', 'predominantly', 'present', 'presented', 'presenting', 'presents', 'presumably', 'previously', 'primarily', 'probably', 'problem', 'problems', 'promptly', 'proud', 'provided', 'provides', 'pt', 'put', 'puts', 'pw', 'py', 'q', 'qa', 'que', 'quickly', 'quite', 'qv', 'r', 'ran', 'rather', 'rd', 're', 'readily', 'really', 'reasonably', 'recent', 'recently', 'ref', 'refs', 'regarding', 'regardless', 'regards', 'related', 'relatively', 'research', 'reserved', 'respectively', 'resulted', 'resulting', 'results', 'right', 'ring', 'ro', 'room', 'rooms', 'round', 'ru', 'run', 'rw', 's', 'sa', 'said', 'same', 'saw', 'say', 'saying', 'says', 'sb', 'sc', 'sd', 'se', 'sec', 'second', 'secondly', 'seconds', 'section', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'sees', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'seventy', 'several', 'sg', 'sh', 'shall', \"shan't\", 'shant', 'she', \"she'd\", \"she'll\", \"she's\", 'shed', 'shell', 'shes', 'should', \"should've\", 'shouldn', \"shouldn't\", 'shouldnt', 'show', 'showed', 'showing', 'shown', 'showns', 'shows', 'si', 'side', 'sides', 'significant', 'significantly', 'similar', 'similarly', 'since', 'sincere', 'site', 'six', 'sixty', 'sj', 'sk', 'sl', 'slightly', 'sm', 'small', 'smaller', 'smallest', 'sn', 'so', 'some', 'somebody', 'someday', 'somehow', 'someone', 'somethan', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specifically', 'specified', 'specify', 'specifying', 'sr', 'st', 'state', 'states', 'still', 'stop', 'strongly', 'su', 'sub', 'substantially', 'successfully', 'such', 'sufficiently', 'suggest', 'sup', 'sure', 'sv', 'sy', 'system', 'sz', 't', \"t's\", 'take', 'taken', 'taking', 'tc', 'td', 'tell', 'ten', 'tends', 'test', 'text', 'tf', 'tg', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that'll\", \"that's\", \"that've\", 'thatll', 'thats', 'thatve', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there'd\", \"there'll\", \"there're\", \"there's\", \"there've\", 'thereafter', 'thereby', 'thered', 'therefore', 'therein', 'therell', 'thereof', 'therere', 'theres', 'thereto', 'thereupon', 'thereve', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'theyd', 'theyll', 'theyre', 'theyve', 'thick', 'thin', 'thing', 'things', 'think', 'thinks', 'third', 'thirty', 'this', 'thorough', 'thoroughly', 'those', 'thou', 'though', 'thoughh', 'thought', 'thoughts', 'thousand', 'three', 'throug', 'through', 'throughout', 'thru', 'thus', 'til', 'till', 'tip', 'tis', 'tj', 'tk', 'tm', 'tn', 'to', 'today', 'together', 'too', 'took', 'top', 'toward', 'towards', 'tp', 'tr', 'tried', 'tries', 'trillion', 'truly', 'try', 'trying', 'ts', 'tt', 'turn', 'turned', 'turning', 'turns', 'tv', 'tw', 'twas', 'twelve', 'twenty', 'twice', 'two', 'tz', 'u', 'ua', 'ug', 'uk', 'um', 'un', 'under', 'underneath', 'undoing', 'unfortunately', 'unless', 'unlike', 'unlikely', 'until', 'unto', 'up', 'upon', 'ups', 'upwards', 'us', 'use', 'used', 'useful', 'usefully', 'usefulness', 'uses', 'using', 'usually', 'uucp', 'uy', 'uz', 'v', 'va', 'value', 'various', 'vc', 've', 'versus', 'very', 'vg', 'vi', 'via', 'viz', 'vn', 'vol', 'vols', 'vs', 'vu', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'wasn', \"wasn't\", 'wasnt', 'way', 'ways', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'web', 'webpage', 'website', 'wed', 'welcome', 'well', 'wells', 'went', 'were', 'weren', \"weren't\", 'werent', 'weve', 'wf', 'what', \"what'd\", \"what'll\", \"what's\", \"what've\", 'whatever', 'whatll', 'whats', 'whatve', 'when', \"when'd\", \"when'll\", \"when's\", 'whence', 'whenever', 'where', \"where'd\", \"where'll\", \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'wheres', 'whereupon', 'wherever', 'whether', 'which', 'whichever', 'while', 'whilst', 'whim', 'whither', 'who', \"who'd\", \"who'll\", \"who's\", 'whod', 'whoever', 'whole', 'wholl', 'whom', 'whomever', 'whos', 'whose', 'why', \"why'd\", \"why'll\", \"why's\", 'widely', 'width', 'will', 'willing', 'wish', 'with', 'within', 'without', 'won', \"won't\", 'wonder', 'wont', 'words', 'work', 'worked', 'working', 'works', 'world', 'would', \"would've\", 'wouldn', \"wouldn't\", 'wouldnt', 'ws', 'www', 'x', 'y', 'ye', 'year', 'years', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'youd', 'youll', 'young', 'younger', 'youngest', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve', 'yt', 'yu', 'z', 'za', 'zero', 'zm', 'zr', 'enron']\n"
     ]
    }
   ],
   "source": [
    "stopwordslist = []\n",
    "\n",
    "with open('stopwords-html.csv', newline='') as csvfile:\n",
    "    csvReader = csv.reader(csvfile)    \n",
    "    for row in csvReader:    \n",
    "        stopwordslist.append(str(row[0]))  \n",
    "        \n",
    "with open('stopwords-text.csv', newline='') as csvfile:\n",
    "    csvReader = csv.reader(csvfile)    \n",
    "    for row in csvReader:    \n",
    "        stopwordslist.append(str(row[0]))   \n",
    "\n",
    "stopwordslist.append(\"enron\")\n",
    "print(len(stopwordslist))\n",
    "print(stopwordslist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc6586",
   "metadata": {},
   "source": [
    "### TF-IDF function which can be used for the spam and ham files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53ac2475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf_stop(dataset):\n",
    "    tfIdfVectorizer=TfidfVectorizer(input='filename', decode_error='ignore', max_features=25, stop_words = stopwordslist)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(dataset) # train to recognize vocabulary\n",
    "#     using pandas dataframe to look at the top 25 values\n",
    "    df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
    "    df = df.sort_values('TF-IDF', ascending=False)\n",
    "    print (df.head(25)) \n",
    "    return tfIdfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390ffd9e",
   "metadata": {},
   "source": [
    "### Run TF-IDF on the ham and spam files (Kitchen-l, lokay-m, GP), basically training the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4c9181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'daren', 'hadn', 'herse', 'himse', 'itse', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "ou          0.563913\n",
      "recipients  0.563372\n",
      "kitchen     0.455128\n",
      "20          0.283567\n",
      "content     0.162416\n",
      "subject     0.143965\n",
      "version     0.086071\n",
      "transfer    0.084770\n",
      "type        0.083709\n",
      "charset     0.080307\n",
      "00          0.043225\n",
      "size        0.004303\n",
      "3d          0.003502\n",
      "solid       0.001681\n",
      "border      0.001441\n",
      "priority    0.001241\n",
      "color       0.000820\n",
      "height      0.000200\n",
      "align       0.000160\n",
      "3dverdana   0.000000\n",
      "nbsp        0.000000\n",
      "quot        0.000000\n",
      "3d2         0.000000\n",
      "mimeole     0.000000\n",
      "1px         0.000000\n"
     ]
    }
   ],
   "source": [
    "stopvectorizer = find_tfidf_stop(tar_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15af2fd",
   "metadata": {},
   "source": [
    "### Run TF-IDF On 1 ham file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52ea6889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              TF-IDF\n",
      "kitchen     0.445435\n",
      "louise      0.445435\n",
      "laura       0.356348\n",
      "luce        0.267261\n",
      "2001        0.178174\n",
      "message     0.178174\n",
      "response    0.178174\n",
      "recipients  0.178174\n",
      "ou          0.178174\n",
      "chicago     0.178174\n",
      "subject     0.178174\n",
      "email       0.178174\n",
      "discuss     0.178174\n",
      "content     0.178174\n",
      "markets     0.089087\n",
      "market      0.089087\n",
      "monday      0.089087\n",
      "noon        0.089087\n",
      "original    0.089087\n",
      "partly      0.089087\n",
      "plain       0.089087\n",
      "politics    0.089087\n",
      "pst         0.089087\n",
      "reached     0.089087\n",
      "mime        0.089087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'daren', 'hadn', 'herse', 'himse', 'itse', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "hamExampleStop = find_tfidf_stop(xtrain[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f1654d",
   "metadata": {},
   "source": [
    "### Run TF-IDF On 1 spam file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6350879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  TF-IDF\n",
      "104             0.304997\n",
      "mimeole         0.304997\n",
      "ceskabar        0.304997\n",
      "priority        0.304997\n",
      "info            0.304997\n",
      "content         0.304997\n",
      "med1cat         0.152499\n",
      "specials        0.152499\n",
      "type            0.152499\n",
      "transfer        0.152499\n",
      "tommie_tate_uk  0.152499\n",
      "tommie          0.152499\n",
      "tiocntdncv      0.152499\n",
      "tate            0.152499\n",
      "subject         0.152499\n",
      "s0ma            0.152499\n",
      "sc0unt          0.152499\n",
      "mime            0.152499\n",
      "produced        0.152499\n",
      "paliourg        0.152499\n",
      "opixmgbgfe      0.152499\n",
      "oct             0.152499\n",
      "normal          0.152499\n",
      "msmail          0.152499\n",
      "v6              0.152499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sethjohnston/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'daren', 'hadn', 'herse', 'himse', 'itse', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spamExampleStop = find_tfidf_stop(xtrain[8000:8001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b765d62",
   "metadata": {},
   "source": [
    "### Transform the TF-IDF vectorizer with the xtrain and xvalid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ff8895",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformedXTrainStop =  stopvectorizer.transform(xtrain) # ytrain set\n",
    "transformedXValidationStop = stopvectorizer.transform(xvalid) # xvalid set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fed2a",
   "metadata": {},
   "source": [
    "###  Train a RandomForestClassifier using the parts 1-9 spam emails and kitchen-l ham emails. Use 100 estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d18e3de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopmodel = RandomForestClassifier(n_estimators=100)\n",
    "stopmodel.fit(transformedXTrainStop, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ae27d",
   "metadata": {},
   "source": [
    "### Using your RandomForestClassifier, predict the emails in GP parts 10-12 and lokay-m. Display the number of true positive (spam), false positive, true negative (ham) and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9b4593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of True Negatives:  2362\n",
      "# of False Positives:  2\n",
      "# of False Negatives:  0\n",
      "# of True Positives:  3423\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      1.00      1.00      2364\n",
      "        spam       1.00      1.00      1.00      3423\n",
      "\n",
      "    accuracy                           1.00      5787\n",
      "   macro avg       1.00      1.00      1.00      5787\n",
      "weighted avg       1.00      1.00      1.00      5787\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypredstop = stopmodel.predict(transformedXValidationStop)\n",
    "# Confusion Matrix and Classification Report\n",
    "tnstop, fpstop, fnstop, tpstop = confusion_matrix(yvalid, ypredstop).ravel()  # flattening the array and list expansion\n",
    "print(\"# of True Negatives: \", tnstop)\n",
    "print(\"# of False Positives: \", fpstop)\n",
    "print(\"# of False Negatives: \", fnstop)\n",
    "print(\"# of True Positives: \", tpstop)\n",
    "\n",
    "print(classification_report(yvalid, ypredstop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be519ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
